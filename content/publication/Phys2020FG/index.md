---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "Multimodal Fusion of Physiological Signals and Facial Action Units forPain Recognition"
authors: [Saurabh Hinduja, Shaun Canavan, Gurmeet Kaur]
date: 2020-05-18T00:44:00-04:00
doi: ""

# Schedule page publish date (NOT publication's date).
publishDate: 2020-04-13T00:44:00-04:00

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["1"]

# Publication name and optional abbreviated publication name.
publication: "Multimodal Fusion of Physiological Signals and Facial Action Units forPain Recognition"
publication_short: "Face and Gesture 2020"

abstract: "In this paper, we propose a method for pain recog-nition by fusing physiological signals (heart rate, respiration,blood pressure, and electrodermal activity) and facial actionunits. We provide experimental validation that the fusion ofthese signals results in a positive impact to the accuracy ofpain recognition, compared to using only one modality (i.e.physiological or action units). These experiments are conductedon subjects from the BP4D+ multimodal emotion corpus, andinclude same- and cross-gender experiments. We also investigatethe correlation between the two modalities to gain furtherinsight into applications of pain recognition. Results suggestthe need for larger and more varied datasets that includephysiological signals and action units that have been codedfor all facial frames."

# Summary. An optional shortened abstract.
summary: "In this paper, we propose a method for pain recog-nition by fusing physiological signals (heart rate, respiration,blood pressure, and electrodermal activity) and facial actionunits. We provide experimental validation that the fusion ofthese signals results in a positive impact to the accuracy ofpain recognition, compared to using only one modality (i.e.physiological or action units). These experiments are conductedon subjects from the BP4D+ multimodal emotion corpus, andinclude same- and cross-gender experiments. We also investigatethe correlation between the two modalities to gain furtherinsight into applications of pain recognition. Results suggestthe need for larger and more varied datasets that includephysiological signals and action units that have been codedfor all facial frames. "

tags: [Affective Computing, Physiolgical Signal, Pain Recognition]
categories: []
featured: true

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
# links:
# - name: Follow
#   url: https://twitter.com
#   icon_pack: fab
#   icon: twitter

url_pdf: http://saurabhh.com/wp-content/uploads/2020/03/FG2020_AMAR_PhysAU_Fusion.pdf
url_code:
url_dataset:
url_poster:
url_project:
url_slides:
url_source:
url_video:

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: "Overview of the Method"
  focal_point: "Center"
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---
